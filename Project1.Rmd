---
title: "Project 1"
author: "Machine Learning Group"
date: "3/5/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#loading libraries
library(ISLR2)
library(glmnet)
```


### Lasso and Ridge


4) Predict the number of applications received using the variables in the university dataset

a)

```{r}
#View(College)

#checking missings 
sum(is.na(College))  #0 


set.seed(1)

#train-test split : 70% training and 30% testing
train <- sample(1:nrow(College), round(nrow(College)*0.7, 0))

test <- (-train)

y.test <- College$Apps[test]

```

b)

```{r}
attach(College)

#linear model
lm.fit <- lm(Apps ~ ., data = College, subset=train)

mean((Apps - predict(lm.fit, College))[test]^2) # 1266407 MSE
```

c)

```{r}
#ridge regression

#glmnet takes an x matrix and y vector
x <- model.matrix(Apps ~ ., College)[, -1]
y <- College$Apps

grid <- 10^seq(10, -2, length=100)

ridge.mod <- glmnet(x[train,], y[train], alpha = 0, lambda = grid, thresh = 1e-12)
cv.out <- cv.glmnet(x[train, ], y[train], alpha = 0)

bestlam <- cv.out$lambda.min

ridge.pred <- predict(ridge.mod, s = bestlam, newx = x[test, ])
mean((ridge.pred - y.test)^2) # 1125215 MSE
```


d)

```{r}
#lasso regression
lasso.mod <- glmnet(x[train,], y[train], alpha = 1, lambda = grid, thresh = 1e-12)
cv.out <- cv.glmnet(x[train, ], y[train], alpha = 1)

bestlam <- cv.out$lambda.min

lasso.pred <- predict(lasso.mod, s = bestlam, newx = x[test, ])
mean((lasso.pred - y.test)^2) #1260855 MSE

#pending: el nÃºmero de estimaciones de coeficiente distintas de cero
```

e)

```{r}
#pending

```



5) 

a)

```{r}
#predict per capita crime rate
attach(Boston)

sum(is.na(Boston))
```


```{r}
#lasso
#create grid of values ranging from 10^10 to 10^-2
grid <- 10^seq(10, -2, length=100)

#put data in correct format
x_lasso <- model.matrix(crim ~ ., Boston)[, -1]
y_lasso <- Boston$crim

#train-test split
train <- sample(1:nrow(x_lasso), nrow(x_lasso)/2)
test <- (-train)
y.test <- y_lasso[test]


library(glmnet)

#fit the model and plot
par(mfrow=c(1,1))
lasso.mod <- glmnet(x_lasso[train, ], y_lasso[train], alpha = 1, lambda = grid)
plot(lasso.mod)

#cross-validation to compute estimated test error
cv.out <- cv.glmnet(x_lasso[train, ], y_lasso[train], alpha = 1)
plot(cv.out)
bestlam <- cv.out$lambda.min
lasso.pred <- predict(lasso.mod, s = bestlam, newx = x_lasso[test, ])
mean((lasso.pred - y.test)^2) #=26.3345

#three of the 10 coefficients are zero
out <- glmnet(x_lasso, y_lasso, alpha = 1, lambda = grid)
lasso.coef <- predict(out, type="coefficients", s = bestlam)[1:13,]
lasso.coef #non-zero variables: zn, indus, chas, nox, rm, dis, rad, ptratio, lstat, medv



#ridge regression

#put data in correct format
x_ridge <- model.matrix(crim ~ ., Boston)[, -1]
y_ridge <- Boston$crim

#train-test split
train <- sample(1:nrow(x_ridge), nrow(x_ridge)/2)
test <- (-train)
y.test <- y_ridge[test]

#fit the model and plot
par(mfrow=c(1,1))
ridge.mod <- glmnet(x_ridge[train, ], y_ridge[train], alpha = 0, lambda = grid)
plot(ridge.mod)

#cross-validation to compute estimated test error
cv.out <- cv.glmnet(x_ridge[train, ], y_ridge[train], alpha = 0)
plot(cv.out)
bestlam <- cv.out$lambda.min
ridge.pred <- predict(ridge.mod, s = bestlam, newx = x_ridge[test, ])
mean((ridge.pred - y.test)^2) #=32.56685 (worse than lasso)

out <- glmnet(x_ridge, y_ridge, alpha = 0, lambda = grid)
ridge.coef <- predict(out, type="coefficients", s = bestlam)[1:13,]
ridge.coef 


```



b)

```{r}
#pending
```





