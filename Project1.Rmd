---
title: "Project 1"
author: "Machine Learning Group"
date: "3/5/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#loading libraries
library(ISLR2)
library(glmnet)
```


### Lasso and Ridge


4) Predecir el número de solicitudes recibidas usando las variables en el conjunto de datos de College

a)

```{r}
#View(College)

#checking missings 
sum(is.na(College))  #0 

set.seed(1)

#train-test split : 70% training and 30% testing
train <- sample(1:nrow(College), round(nrow(College)*0.7, 0))

test <- (-train)

y.test <- College$Apps[test]

```

b)

```{r}
attach(College)

#linear model
lm.fit <- lm(Apps ~ ., data = College, subset=train)

MSE_ols <-mean((Apps - predict(lm.fit, College))[test]^2) # 1266407 MSE
MSE_ols
```

c)

```{r}
#ridge regression

#glmnet takes an x matrix and y vector
x <- model.matrix(Apps ~ ., College)[, -1]
y <- College$Apps

grid <- 10^seq(10, -2, length=100)

ridge.mod <- glmnet(x[train,], y[train], alpha = 0, lambda = grid, thresh = 1e-12)
cv.out <- cv.glmnet(x[train, ], y[train], alpha = 0)

bestlam <- cv.out$lambda.min

ridge.pred <- predict(ridge.mod, s = bestlam, newx = x[test, ])

MSE_ridge <- mean((ridge.pred - y.test)^2) # 1125215 MSE
MSE_ridge
```


d)

```{r}
#lasso regression
lasso.mod <- glmnet(x[train,], y[train], alpha = 1, lambda = grid, thresh = 1e-12)
cv.out <- cv.glmnet(x[train, ], y[train], alpha = 1)

bestlam <- cv.out$lambda.min

lasso.pred <- predict(lasso.mod, s = bestlam, newx = x[test, ])

MSE_lasso <- mean((lasso.pred - y.test)^2) #1260855 MSE
MSE_lasso
```


```{r}

#el número de estimaciones de coeficiente distintas de cero

# Train the data using different lambdas values
out=glmnet( x , y , alpha=1, lambda=grid, thresh = 1e-12 ) 

# Get coefficients 
lasso.coef = predict( lasso.mod  , type="coefficients" , s=bestlam )[1:18,]

# Get coefficients  != 0 
lasso.coef[lasso.coef!=0]
```


e)


```{r}

data.frame(MSE_ols, MSE_ridge, MSE_lasso)
```


5) 

a)

```{r}
#predict per capita crime rate
attach(Boston)

sum(is.na(Boston))

set.seed(1)


```


```{r}
#lasso
#create grid of values ranging from 10^10 to 10^-2
grid <- 10^seq(10, -2, length=100)

#put data in correct format
x_lasso <- model.matrix(crim ~ ., Boston)[, -1]
y_lasso <- Boston$crim

#train-test split
train <- sample(1:nrow(x_lasso), nrow(x_lasso)/2)
test <- (-train)
y.test <- y_lasso[test]

#fit the model 
lasso.mod <- glmnet(x_lasso[train, ], y_lasso[train], alpha = 1, lambda = grid)

#cross-validation to compute estimated test error
cv.out <- cv.glmnet(x_lasso[train, ], y_lasso[train], alpha = 1)

bestlam <- cv.out$lambda.min

lasso.pred <- predict(lasso.mod, s = bestlam, newx = x_lasso[test, ])

mean((lasso.pred - y.test)^2) #=41.0081


out <- glmnet(x_lasso, y_lasso, alpha = 1, lambda = grid)

lasso.coef <- predict(out, type="coefficients", s = bestlam)[1:13,]
lasso.coef #non-zero variables: zn, indus, chas, nox, rm, dis, rad, tax, ptratio, lstat, medv

           #zero variable: age

```


```{r}
#ridge regression

#put data in correct format
x_ridge <- model.matrix(crim ~ ., Boston)[, -1]
y_ridge <- Boston$crim

#train-test split
train <- sample(1:nrow(x_ridge), nrow(x_ridge)/2)
test <- (-train)
y.test <- y_ridge[test]

#fit the model 
ridge.mod <- glmnet(x_ridge[train, ], y_ridge[train], alpha = 0, lambda = grid)

#cross-validation to compute estimated test error
cv.out <- cv.glmnet(x_ridge[train, ], y_ridge[train], alpha = 0)

bestlam <- cv.out$lambda.min

ridge.pred <- predict(ridge.mod, s = bestlam, newx = x_ridge[test, ])
mean((ridge.pred - y.test)^2) #=54.77138 (worse than lasso)

out <- glmnet(x_ridge, y_ridge, alpha = 0, lambda = grid)
ridge.coef <- predict(out, type="coefficients", s = bestlam)[1:13,]
ridge.coef 
```


b) 

El mejor modelo es lasso, ya que, a diferencia de ridge, este tiene un error de predicción menor (41 vs 55). El modelo lasso propuesto no incluye a todas las variables. De hecho, solo excluye a 1 de estas, la cual es edad.





