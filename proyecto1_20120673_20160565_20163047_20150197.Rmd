---
title: "Proyecto 1"
#author: "Machine Learning Group"
#date: "3/5/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Integrantes:**
Luciana Figueroa Valdivia (20120673),
Chiara Zamora Mendoza (20160565),
Yoseph Ayala Valencia (20163047),
Claudia Villena Tagle (20150197)


```{r}
#loading libraries
library(ISLR2)
library(glmnet)
library(MASS)
library(boot)
```


### Bootstrap: 

**1) Para una muestra de tamaño 5 (n=5), ¿Cual es la probabilidad que la j-enesima observacion se encuentre dentro de la muestra bootstrap?**

Si primero vemos la probabibilidad de que la observación seleccionada para el bootstrap *no* sea la j-enésima observación de la data original, tenemos lo siguiente: 

$$1-(1/n)$$

Esto es porque tenemos n observaciones por escoger de la data original y todas tienen igual de probabilidad de ser seleccionadas, por lo que su complemento (probabilidad de ser seleccionado) también sería igual.
Por otro lado, en bootstrap sabemos que se hace una toma de muestra *con reposición*, por lo que cada muestra es independiente respecto a la otra, entonces, la probabilidad de que ninguna de las observaciones tomadas para la muestra sean la j-enésima observación del dataset, será la multiplicación de probabilidades, lo que nos da el siguiente resultante:

$$(1-1/n)^n$$

En este caso, nos están pidiendo encontrar la probabilidad de que la j-enésima observación *se encuentre* dentro de la muestra bootstrap, entonces podemos tomar el complemento de la formula mencionada previamente: 

$$1 - (1-1/n)^n$$

Sabemos que n = 5, entonces nos quedaríamos con:

$$1 - (1-1/5)^5 = 0.672$$

Entonces, podemos concluir que la probabilidad de que la j-enésima observación se encuentre en una muestra bootstrap de n = 5 es aproximadamente 0.672.


**2) Genere un plot que muestre en el eje X el tamaño de muestra (n) desde 1 a 100000 y en el eje Y la probabilidad que la j-enesima observación se encuentre dentro de la muestra bootstrap. Comente los resultados.**

**Plot 1**

```{r}
prob <- numeric(1e+05)
x <- numeric(1e+05)
for (i in 1:1e+05) {
    prob[i] <- 1 - ((i - 1)/i)^i
    x[i] <- i
}

plot(x, prob, type = "l")
```

**Plot 2**

```{r}
x <- 1:100000
y <- 1-(1-(1/x))^x
plot(x, y, log="x", type="l", xlab="n", col="red", xaxt="n", ylab="1 - ( 1 - (1/n) )^n", ylim=c(0, 1))
ticks <- seq(0, 5, by=1)
labels <- sapply(ticks, function(i) as.expression(bquote(10^ .(i))))
axis(1, at=c(1, 10, 100, 1000, 10000,100000), labels=labels)
```

* **Comentario:** A medida que n -esto es, el tamaño de la muestra- crece infinitamente, se observa en el gráfico que las probabilidades de que la j-enesima observación se encuentre dentro de la muestra bootstrap convergen en un valor (*0.632*, equivalente a 1−(1/e)). 


**3) Ahora consideraremos el conjunto de datos de vivienda de Boston, del ISLR2 library.**

**a) Con base en este conjunto de datos, proporcione una estimación de la media de la variable medv. Nombre a esta estimación u.**

```{r}
#a)
u = mean(Boston$medv)
u
```

La media de la variable *medv* es 22.53 aproximadamente. 
En el contexto del problema, esto significa que la mediana promedio del valor de los hogares ocupados es 22.53 mil dólares.

**b) Proporcione una estimación del error estándar de u.**

```{r}
#b)
se_mu = sd(Boston$medv) / sqrt(nrow(Boston))
se_mu
```

El error estándar estimado para la variable *medv* es 0.41 aproximadamente.

**c) Ahora estime el error estándar de u, usando bootstrap.**

```{r}
#c)
set.seed(100)
fun_se = function(data, index) {
    u = mean(data[index])
    return (u)
}
boot(Boston$medv, fun_se, 1000)
```

Podemos ver que el resultado de estimar el error estándar de mu, usando bootstrap (0.419) es similar a lo hallado en la pregunta 3b (0.409).

**d) Con base en su estimación en (c) , proporcione un intervalo de confianza al 95 % de la media de medv utilizando bootstrap. Compárelo con los resultados obtenidos usando t.test(Boston$medv).**

```{r}
#d)

#Hallamos el intervalo de confianza con los resultados obtenidos en la parte c:

intervalo_u = c(22.53 - 2 * 0.4192063 , 22.53 + 2 * 0.4192063)
intervalo_u
```

```{r}
#Obtenemos los resultados con t.test

t.test(Boston$medv)
```

Vemos que el intervalo obtenido por bootstrap [21.61 ; 23.37] es bastante similar al resultante de la función t.test [21.73 ; 23.34]

**e) Con base en este conjunto de datos, proporcione una estimación, u_med, para la mediana de la variable medv.**

```{r}
u_med = median(Boston$medv)
u_med
```

La mediana estimada para la variable *medv* es 21.2 (en miles de dólares).

**f) Estime el error estándar de u_med utilizando bootstrap. Comente sus hallazgos.**

```{r}
set.seed(20)
fun_se <- function(data, index) {
    u <- median(data[index])
    return (u)
}
boot(Boston$medv, fun_se, 1000)
```

En este caso usamos bootstrap para obtener el error estándar de la mediana, y vemos que este es 0.38 aproximadamente, lo cual viene a ser un valor pequeño a comparación del valor de la mediana estimado (21.2, igual a lo obtenido en la parte e).

### Cross Validation:

**1) Explique cómo se implementa el k-fold cross validation.**

Para hacer k-fold cross validation, debemos dividir nuestro dataset en k grupos de igual tamaño; Estos grupos no pueden tener la misma observación entre ellos.
De estos k grupos, escogeremos todos menos uno (k-1) como set de entrenamiento, y el restante será set de test. 
Vamos a iterar k veces para obtener el modelo $M1, M2, ...Mk$, donde cada modelo $Mj$ se obtendrá usando todas las observaciones menos las del grupo $j$, ya que este grupo es el de validación.
Con dicho test de validación, podemos obtener el score de accuracy, que puede ser el error cuadrático medio en caso el modelo sea un modelo de predicción, o una porporción de observaciones clasificadas correctamente, en caso hablemos de un modelo de clasificación. 
Luego de obtener el score para cada modelo, se toma el promedio de los resultados para obtener el score de accuracy final.

**2) Comente las ventajas y desventajas de k-fold cross validation con respecto a validation set approach y LOOCV.**

 - Comparación contra *validation set approach*: 
 La ventaja de k-fold contra este método es que dado a que se estiman varios modelos y son sus resultados los que se promedian, la variabilidad del score de accuracy es mucho menor que el que se obtiene por el validation set approach, ya que este último depende de que observaciones entran al set de entrenamiento y cuales van al set de vaildación.
 Dicho esto, la desventaja de K-folds es por temas computacionales, como debe iterar tantos modelos como k folios hayan, esto puede hacer el proceso más pesado. 
 - Comparación contra *LOOCV*: 
 Una ventaja contra LOOCV es por temas computaciones, debido a que normalmente k-fold usualmente requiere menos grupos, LOOCV requiere k = n, donde n es el número de observaciones en el dataset. Dado al número de modelos a ajustar, el estimado del score de accuracy llega a ser más insesgado que el obtenido por k-folds, sin embargo, los resultados del score de cada modelo obtenido por LOOCV son altamente correlacionados, ya que usan practicamente los mismos datos (n-1 observaciones del dataset), esto termina aumentando la variabilidad del score contra la variabilidad que se obtiene con k-folds, lo que finalmente hace el resultado del score de k-folds mas certero.


### Lasso y Ridge

**1) Determine cuál de las siguientes proposiciones es verdadera. Justifique su respuesta.**

**a) Lasso con respecto a OLS es:**

* Menos flexible y, por lo tanto, mejorará la precisión de la predicción cuando el aumento
en el sesgo es menor que la disminución en la varianza de las predicciones.

* **Justificación:** La solución de Lasso puede tener una reducción en la varianza a expensas de un pequeño aumento en el sesgo, mientras que las estimaciones de mínimos cuadrados tienen una varianza alta. Lasso puede reducir las estimaciones de coeficientes, eliminando variables no esenciales para obtener menos varianza y un mayor sesgo*.

**b) Evalue (a) para Ridge con respecto a OLS.**

* Menos flexible y, por lo tanto, mejorará la precisión de la predicción cuando el aumento
en el sesgo es menor que la disminución en la varianza de las predicciones.

* **Justificación:** Al igual que Lasso, Ridge puede reducir las estimaciones de los coeficientes, disminuyendo la varianza con un mayor sesgo. Ridge es menos flexible que los mínimos cuadrados. A diferencia de Lasso, sin embargo, no elimina variables no esenciales, sino que reduce sus coeficientes con tendencia a 0x. 

 
**2) Supongamos que estimamos los betas en el siguiente modelo de regresión lineal. Determine cúal de las siguientes proposiciones es verdadera**

**a) A medida que aumentamos s desde 0, el RSS de la data de entrenamiento:**

* Disminuye constantemente

* **Justificación:** A medida que s aumenta, los estimadores de coeficientes tendrán que ser menores e iguales que este, por tanto, igual de grandes. Esto genera que la restricción sobre los betas disminuya. De esta forma, podría presentarse la solución de los mínimos cuadrados (la función que mejor se aproxime a los datos) y, por tanto, el RSS de la data entrenamiento será más baja. Además, al observar el RSS de nuestra data de entrenamiento, se espera que esta vaya reduciendo conforme se vaya entrenando con ella. Una disminución continua.


**b) A medida que aumentamos s desde 0, el RSS de la data de test:**

* Disminuye inicialmente y luego eventualmente comienza a aumentar en forma de U

* **Justificación:** En el caso del RSS en la data de prueba, se espera que este error disminuya para luego aumentar en forma de U. Esto se debe a que en cuanto aumente más el s, lleva a que los estimadores de coeficientes también aumenten. Como se comentó previamente, llegará el punto en el que se presente la solución de los mínimos cuadrados, llegando a reducirse el RSS; después aumenta progresivamente en tanto los valores de beta se ajustan a la data de entrenamiento y se genera un evento de overfiting en la data de test. 


**3) Supongamos que estimamos los betas en el siguiente modelo de regresión lineal. Determine cúal de las siguientes proposiciones es verdadera**

**a) A medida que aumentamos λ desde 0, el RSS de la data de entrenamiento:**

* Aumenta constantemente

* **Justificación:** Este segundo término de la función es el shrinkage penalty. Por tanto, cuando λ es 0 la penalidad carece de efecto, permitiendo que la regresión produzca las estimaciones de mínimos cuadrados. En este ejercicio, conforme se aumenta λ el efecto de la penalidad aumenta y la restricción de beta también lo hace, acercándonos a valores de B menores. Por ello, cuando λ aumenta, el RSS en el grupo de entrenamiento aumentará constantemente. 


**b) A medida que aumentamos λ desde 0, el RSS de la data de test:**

* Disminuye inicialmente y luego eventualmente comienza a aumentar en forma de U

* **Justificación:** En este caso, la segunda parte de la fórmula controla la complejidad/flexibilidad del modelo con el objetivo de reducir el error de generalización y evitar overfiting. Cuando lambda es reducido se optimiza al OLS normal con un RSS reducido, conforme nos acercamos al mejor valor de entrenamiento. 
Conforme lambda aumenta se pierde flexibilidad del modelo, reduciendo la varianza en las predicciones y aumentando el sesgo. Cuando aumenta la capacidad de predicción del modelo disminuye el RSS de la data de test. No obstante, se llega a un punto en el que aumenta más el sesgo que lo que se reduce la varianza del modelo, produciendo un mayor sesgo. Esto genera un aumento en el RSS de la data de test. 


**4) Predecir el número de solicitudes recibidas usando las variables en el conjunto de datos de College**

**a) Divida el conjunto de datos en un conjunto de entrenamiento (70 %) y un conjunto de prueba (30 %)**

```{r}
#View(College)

#checking missings 
sum(is.na(College))  #0 

set.seed(1)

#train-test split : 70% training and 30% testing
train <- sample(1:nrow(College), round(nrow(College)*0.7, 0))

test <- (-train)

y.test <- College$Apps[test]

```

**b) Ajuste un modelo lineal usando OLS en el conjunto de entrenamiento, y reportar el MSE del conjunto de prueba**

```{r}
attach(College)

#linear model
lm.fit <- lm(Apps ~ ., data = College, subset=train)

MSE_ols <-mean((Apps - predict(lm.fit, College))[test]^2) # 1266407 MSE
MSE_ols
```

**c) Ajuste un modelo Ridge en el conjunto de entrenamiento, con λ elegido por cross validation. Reporte el MSE del conjunto de prueba.**

```{r}
#ridge regression

#glmnet takes an x matrix and y vector
x <- model.matrix(Apps ~ ., College)[, -1]
y <- College$Apps

grid <- 10^seq(10, -2, length=100)

ridge.mod <- glmnet(x[train,], y[train], alpha = 0, lambda = grid, thresh = 1e-12)
cv.out <- cv.glmnet(x[train, ], y[train], alpha = 0)

bestlam <- cv.out$lambda.min

ridge.pred <- predict(ridge.mod, s = bestlam, newx = x[test, ])

MSE_ridge <- mean((ridge.pred - y.test)^2) # 1125215 MSE
MSE_ridge
```


**d) Ajuste un modelo Lasso en el conjunto de entrenamiento, con λ elegido por cross validation. Reporte el MSE del conjunto prueba, junto con el número de estimaciones de coeficiente distintas de cero.**


```{r}
#lasso regression
lasso.mod <- glmnet(x[train,], y[train], alpha = 1, lambda = grid, thresh = 1e-12)
cv.out <- cv.glmnet(x[train, ], y[train], alpha = 1)

bestlam <- cv.out$lambda.min

lasso.pred <- predict(lasso.mod, s = bestlam, newx = x[test, ])

MSE_lasso <- mean((lasso.pred - y.test)^2) #1234664 MSE
MSE_lasso
```


```{r}

#el número de estimaciones de coeficiente distintas de cero

# Train the data using different lambdas values
out=glmnet( x , y , alpha=1, lambda=grid, thresh = 1e-12 ) 

# Get coefficients 
lasso.coef = predict( lasso.mod  , type="coefficients" , s=bestlam )[1:18,]

# Get coefficients  != 0 
lasso.coef[lasso.coef!=0]
```


**e) Muestre un dataframe que resuma los resultados**


```{r}

data.frame(MSE_ols, MSE_ridge, MSE_lasso)
```


**5) Ahora intentaremos predecir la tasa de criminalidad per cápita en los datos de Boston, del ISLR2 library.**

**a) Proponga un modelo (o conjunto de modelos) que performen bien en este conjunto de datos y justifique su respuesta. Asegúrese de que estos modelos evaluen el rendimiento del modelo utilizando cross validation, en lugar de utilizar el error de entrenamiento.**

Inicialmente, proponemos dos modelos: una regresión lasso y una regresión ridge. 

```{r}
#predict per capita crime rate
attach(Boston)

sum(is.na(Boston))

set.seed(1)


```

**Lasso**

```{r}
#lasso
#create grid of values ranging from 10^10 to 10^-2
grid <- 10^seq(10, -2, length=100)

#put data in correct format
x_lasso <- model.matrix(crim ~ ., Boston)[, -1]
y_lasso <- Boston$crim

#train-test split
train <- sample(1:nrow(x_lasso), nrow(x_lasso)/2)
test <- (-train)
y.test <- y_lasso[test]

#fit the model 
lasso.mod <- glmnet(x_lasso[train, ], y_lasso[train], alpha = 1, lambda = grid)

#cross-validation to compute estimated test error
cv.out <- cv.glmnet(x_lasso[train, ], y_lasso[train], alpha = 1)

bestlam <- cv.out$lambda.min

lasso.pred <- predict(lasso.mod, s = bestlam, newx = x_lasso[test, ])

mean((lasso.pred - y.test)^2) #=40.89875


out <- glmnet(x_lasso, y_lasso, alpha = 1, lambda = grid)

lasso.coef <- predict(out, type="coefficients", s = bestlam)[1:13,]
lasso.coef #non-zero variables: zn, indus, chas, nox, rm, dis, rad, ptratio, lstat, medv

           #zero variable: age, tax

```

**Ridge**

```{r}
#ridge regression

#put data in correct format
x_ridge <- model.matrix(crim ~ ., Boston)[, -1]
y_ridge <- Boston$crim

#train-test split
train <- sample(1:nrow(x_ridge), nrow(x_ridge)/2)
test <- (-train)
y.test <- y_ridge[test]

#fit the model 
ridge.mod <- glmnet(x_ridge[train, ], y_ridge[train], alpha = 0, lambda = grid)

#cross-validation to compute estimated test error
cv.out <- cv.glmnet(x_ridge[train, ], y_ridge[train], alpha = 0)

bestlam <- cv.out$lambda.min

ridge.pred <- predict(ridge.mod, s = bestlam, newx = x_ridge[test, ])
mean((ridge.pred - y.test)^2) #=58.32772 (worse than lasso)

out <- glmnet(x_ridge, y_ridge, alpha = 0, lambda = grid)
ridge.coef <- predict(out, type="coefficients", s = bestlam)[1:13,]
ridge.coef 
```

Dado que el modelo que nos da un menor error de predicción es lasso, nos quedamos con este. 

**b) ¿El modelo elegido involucra todas las variables disponibles? Justifique.**

El mejor modelo es lasso, ya que, a diferencia de ridge, este tiene un error de predicción menor (41 vs 58). El modelo lasso propuesto no incluye a todas las variables. De hecho, excluye a 2 de estas: age y tax. Esto debido a que, dado el parámetro de regularización del modelo, lasso tiende a excluir variables del modelo, ya que les otorga un coefiente de cero. Para este caso en particular, solo se están excluyendo las dos variables anteriomente mencionadas.


