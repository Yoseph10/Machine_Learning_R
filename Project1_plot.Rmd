
2. Genere un plot que muestre en el eje X el tamaño de muestra (n) desde 1 a 100000 y en el eje Y la probabilidad que la j-enesima observación se encuentre dentro de la muestra bootstrap. Comente los resultados.

**Plot 1**

```{r}
prob <- numeric(1e+05)
x <- numeric(1e+05)
for (i in 1:1e+05) {
    prob[i] <- 1 - ((i - 1)/i)^i
    x[i] <- i
}

plot(x, prob, type = "l")
```

**Plot 2**

```{r}
x <- 1:100000
y <- 1-(1-(1/x))^x
plot(x, y, log="x", type="l", xlab="n", col="red", xaxt="n", ylab="1 - ( 1 - (1/n) )^n", ylim=c(0, 1))
ticks <- seq(0, 5, by=1)
labels <- sapply(ticks, function(i) as.expression(bquote(10^ .(i))))
axis(1, at=c(1, 10, 100, 1000, 10000,100000), labels=labels)
```

* **Comentario:** A medida que n -esto es, el tamaño de la muestra- crece infinitamente, se observa en el gráfico que las probabilidades de que la j-enesima observación se encuentre dentro de la muestra bootstrap convergen en un valor (*0.632*, equivalente a 1−(1/e)). 

**Ridge y Lasso**

**1. Determine cuál de las siguientes proposiciones es verdadera. Justifique su respuesta.**

a) Lasso con respecto a OLS es:

* Menos flexible y, por lo tanto, mejorará la precisión de la predicción cuando el aumento
en el sesgo es menor que la disminución en la varianza de las predicciones.

* **Justificación:** La solución de Lasso puede tener una reducción en la varianza a expensas de un pequeño aumento en el sesgo, mientras que las estimaciones de mínimos cuadrados tienen una varianza alta. Lasso puede reducir las estimaciones de coeficientes, eliminando variables no esenciales para obtener menos varianza y un mayor sesgo*.

b) Evalue (a) para Ridge con respecto a OLS.
* **Justificación:** Al igual que Lasso, Ridge puede reducir las estimaciones de los coeficientes, disminuyendo la varianza con un mayor sesgo. Ridge es menos flexible que los mínimos cuadrados. A diferencia de Lasso, sin embargo, no elimina variables no esenciales, sino que reduce sus coeficientes con tendencia a 0x. 
