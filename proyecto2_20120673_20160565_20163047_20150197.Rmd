---
title: "Proyecto 2"
#author: ""
#date: "3/15/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Integrantes:**

Luciana Figueroa Valdivia (20120673),
Chiara Zamora Mendoza (20160565),
Yoseph Ayala Valencia (20163047),
Claudia Villena Tagle (20150197)


```{r}
#loading libraries
library(ISLR2)
library(glmnet)
library(MASS)
library(boot)
```

**1)   **


**2)   **

**Enfoque del voto mayoritario**

Consideraremos como punto de corte al 0.5. Es decir, si es que hay más cantidad de valores que sean mayor o igual a 0.5, ello significa que el voto mayoritario indica que la predicción nos da rojo. Si es que es menor a 0.5, la predicción nos da verde.
Encontramos que 6 valores son mayores o iguales a nuestro punto de corte (0.5) y que 4 son menores. De esta forma, 6 > 4 lo que implica que, bajo este enfoque, la predicción nos debe dar rojo.

**Enfoque de probabilidad media**

Para este enfoque, solo debemos promediar los valores. Realizando ello, obtenemos un valor de 0.45. Al ser este menor que nuestro punto de corte (0.5), nos indicaría que la predicción es verde bajo este enfoque.


```{r}
#creamos el vector con el conjunto de valores
r <-  c(0.1, 0.15, 0.2, 0.2, 0.55, 0.6, 0.6, 0.65, 0.7, 0.75)

#Enfoque del voto mayoritario
R <- sum(r >= 0.5)
R

G <- sum(r < 0.5)
G

R > G #TRUE

#Enfoque de probabilidad media
mean(r) #0.45

```


**3)   **
Para el caso de los árboles de decisión tenemos dos posibilidades, los de clasificación y los de regresión. En el caso de clasificación usamos el índice de gini o el de entropía como criterio de decisión para las particiones óptimas que se harán en el árbol.<br/>
En el caso de los arboles de regresión, que es en el cual nos concentraremos, se usa mínimos cuadrados. Buscamos las particiones que reduzcan la suma de cuadrados de los residuos, o en inglés _Residual Sum Square (RSS)_, entre la observación y la media de cada nodo encontrado. 
Dado el número de variables que normalmente se usan para este algoritmo, es casi imposible tomar en cuenta todas las particiones que se pueden armar, por eso se hace uso de un método de partición recursivo, denominado en inglés como _recursive binary splitting_. <br/>
Lo que hace este método es comenzar desde el tope del árbol y selecciona la variable que dividirá a la base en dos grupos, la variable seleccionada será aquella que reduzca el _RSS_; Luego de encontrar la variable ideal y tener dos niveles nuevos, pasa a abrir estos dos nodos hallados y volver a dividirlos en dos a cada uno, hallando una nueva variable que siga minimizando el _RSS_. Lo importante de este método es que es recursivo ya que va a intentar todas las variables posibles en el *mismo* nivel, es decir, no pasará a la siguiente apertura de los nodos si es que no está seguro de haber encontrado a variable adecuada para la partición. <br/>
Este proceso se repetirá hasta que quede un número mínimo de observaciones dentro de los nodos finales. <br/>
Al tener el árbol final tenemos que tomar en cuenta que se puede presentar el problema de sobreajuste. Dada a la cantidad de variables y sub conjuntos de árboles que se pueden obtener dentro del árbol obtenido por el método de _recursive binary splitting_, es muy costoso hacer validación cruzada, es por eso que el paso final es hacer un podado del árbol, denominado en inglés como _pruning_, que introduce un parámetro $\alpha$ que permite que se encuentre los mejores subsets de árboles que obtienen el mejor ajuste de la data de entrenamiento con la menor cantidad de niveles posibles. Para hallar esto, se busca reducir la ecuación de:

$$\sum_{m=i}^{|T|}\sum_{i:x_i \in R_m} (y_i - \hat{y}_{R_m})^2 + \alpha|T|$$

En este caso, $|T|$ es el número de nodos terminales del arbol $T$, considerando que si $\alpha$ = 0 tendríamos el árbol obtenido originalmente, a medida que aumente $\alpha$ el árbol se "podará" más. El $R_m$ es la partición correspondiente al nodo terminal $m$ y el $\hat{y}_{R_m}$ es la media de las observaciones del set de entrenamiento que se encuentran en la partición $R_m$.<br/>
Para obtener el $\alpha$ ideal, haremos una validación de k-fólios y al obtener este, volvemos al dataset completo para obtener el árbol podado correspondiente a este $\alpha$ ideal. 


**4)   **


**5)   **

```{r}

library(MASS)
library(randomForest)

set.seed(1)

#empezamos a crear train y tests simples

n <- nrow(Boston)
train.set <- sample(1:n,n/2)

Boston.train <- Boston[train.set, -14]
Boston.test <- Boston[-train.set, -14]

Y.train <- Boston[train.set, 14]
Y.test <- Boston[-train.set, 14]

#Creamos los distintos random forest con distintos mtry

rf.a <- randomForest(Boston.train , Y.train, Boston.test, Y.test, mtry = ncol(Boston.train), ntree = 5000)
rf.b <- randomForest(Boston.train , Y.train, Boston.test, Y.test, mtry = ncol(Boston.train) / 2, ntree = 5000)
rf.c <- randomForest(Boston.train , Y.train, Boston.test, Y.test, mtry = sqrt(ncol(Boston.train)), ntree = 5000)

#Gráfico que muestra los errores de prueba

library(ggplot2)
ggplot() + 
  geom_line(aes(x = c(1:5000), y = rf.a$test$mse, color = "M = P")) +
  geom_line(aes(x = c(1:5000), y = rf.b$test$mse, color = "M = P/2")) +
  geom_line(aes(x = c(1:5000), y = rf.c$test$mse, color = "M = Sqrt(P)")) +
  theme_classic() + 
  xlab("Number of Trees") + ylab("Test Classification Error") + scale_colour_discrete(name="Form")

rf.a$test$mse[5000]

rf.b$test$mse[5000]

rf.c$test$mse[5000]


```

