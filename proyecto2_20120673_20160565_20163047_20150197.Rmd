---
title: "Proyecto 2"
#author: ""
#date: "3/15/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Integrantes:**

Luciana Figueroa Valdivia (20120673),
Chiara Zamora Mendoza (20160565),
Yoseph Ayala Valencia (20163047),
Claudia Villena Tagle (20150197)


```{r}
#loading libraries
library(ISLR2)
library(glmnet)
library(MASS)
library(boot)
library(tidyverse)
library(tibble)
library(ggplot2)
```

**1)   **

1. Considere el índice de Gini, el error de clasificación y la entropía en un árbol de clasificación con
dos clases. Cree una sola gráfica que muestre cada una de estas cantidades como una función de
pˆm1. El eje x debe mostrar pˆm1, con un rango de 0 a 1, y el eje y debe mostrar el valor del índice
de Gini, el error de clasificación y la entropía.

* *Índice de Gini*

$$G=\sum_{k=1}^K\hat p_{mk}(1 - \hat p_{mk})$$

* *Entropía*

$$D=-\sum_{k=1}^K\hat p_{mk}\text{log} \hat p_{mk}$$

```{r}
p = seq(0, 1, 0.01)

indice_gini = 2 * p * (1 - p)

error_clasificacion = 1 - pmax(p, 1 - p)

entropia = - (p * log(p) + (1 - p) * log(1 - p))
```

```{r}
matplot(p, 
        cbind(indice_gini, error_clasificacion, entropia), 
        pch = c(1,2,3),
        col = c("darkblue", "brown", "darkolivegreen4"),
        ylab = "Gini index, class. error, entropy",
        type = 'b')
legend('bottom', inset=.01, legend = c('Índice de Gini', 'Error de clasificación', 'Entropía'), 
       col = c("darkblue", "brown", "darkolivegreen4" ), 
       pch=c(1,2,3))
```

**2)   **

**Enfoque del voto mayoritario**

Consideraremos como punto de corte al 0.5. Es decir, si es que hay más cantidad de valores que sean mayor o igual a 0.5, ello significa que el voto mayoritario indica que la predicción nos da rojo. Si es que es menor a 0.5, la predicción nos da verde.
Encontramos que 6 valores son mayores o iguales a nuestro punto de corte (0.5) y que 4 son menores. De esta forma, 6 > 4 lo que implica que, bajo este enfoque, la predicción nos debe dar rojo.

**Enfoque de probabilidad media**

Para este enfoque, solo debemos promediar los valores. Realizando ello, obtenemos un valor de 0.45. Al ser este menor que nuestro punto de corte (0.5), nos indicaría que la predicción es verde bajo este enfoque.


```{r}
#creamos el vector con el conjunto de valores
r <-  c(0.1, 0.15, 0.2, 0.2, 0.55, 0.6, 0.6, 0.65, 0.7, 0.75)

#Enfoque del voto mayoritario
R <- sum(r >= 0.5)
R

G <- sum(r < 0.5)
G

R > G #TRUE

#Enfoque de probabilidad media
mean(r) #0.45

```


**3)   **
Para el caso de los árboles de decisión tenemos dos posibilidades, los de clasificación y los de regresión. En el caso de clasificación usamos el índice de gini o el de entropía como criterio de decisión para las particiones óptimas que se harán en el árbol.<br/>
En el caso de los arboles de regresión, que es en el cual nos concentraremos, se usa mínimos cuadrados. Buscamos las particiones que reduzcan la suma de cuadrados de los residuos, o en inglés _Residual Sum Square (RSS)_, entre la observación y la media de cada nodo encontrado. 
Dado el número de variables que normalmente se usan para este algoritmo, es casi imposible tomar en cuenta todas las particiones que se pueden armar, por eso se hace uso de un método de partición recursivo, denominado en inglés como _recursive binary splitting_. <br/>
Lo que hace este método es comenzar desde el tope del árbol y selecciona la variable que dividirá a la base en dos grupos, la variable seleccionada será aquella que reduzca el _RSS_; Luego de encontrar la variable ideal y tener dos niveles nuevos, pasa a abrir estos dos nodos hallados y volver a dividirlos en dos a cada uno, hallando una nueva variable que siga minimizando el _RSS_. Lo importante de este método es que es recursivo ya que va a intentar todas las variables posibles en el *mismo* nivel, es decir, no pasará a la siguiente apertura de los nodos si es que no está seguro de haber encontrado a variable adecuada para la partición. <br/>
Este proceso se repetirá hasta que quede un número mínimo de observaciones dentro de los nodos finales. <br/>
Al tener el árbol final tenemos que tomar en cuenta que se puede presentar el problema de sobreajuste. Dada a la cantidad de variables y sub conjuntos de árboles que se pueden obtener dentro del árbol obtenido por el método de _recursive binary splitting_, es muy costoso hacer validación cruzada, es por eso que el paso final es hacer un podado del árbol, denominado en inglés como _pruning_, que introduce un parámetro $\alpha$ que permite que se encuentre los mejores subsets de árboles que obtienen el mejor ajuste de la data de entrenamiento con la menor cantidad de niveles posibles. Para hallar esto, se busca reducir la ecuación de:

$$\sum_{m=i}^{|T|}\sum_{i:x_i \in R_m} (y_i - \hat{y}_{R_m})^2 + \alpha|T|$$

En este caso, $|T|$ es el número de nodos terminales del arbol $T$, considerando que si $\alpha$ = 0 tendríamos el árbol obtenido originalmente, a medida que aumente $\alpha$ el árbol se "podará" más. El $R_m$ es la partición correspondiente al nodo terminal $m$ y el $\hat{y}_{R_m}$ es la media de las observaciones del set de entrenamiento que se encuentran en la partición $R_m$.<br/>
Para obtener el $\alpha$ ideal, haremos una validación de k-fólios y al obtener este, volvemos al dataset completo para obtener el árbol podado correspondiente a este $\alpha$ ideal. 


**4)   **

Si bien en las indicaciones se nos pide que mtry estén entre el rango de 5 a 40 con saltos de 5, solo hemos considerado los casos cuando mtry es 5 y 10. Cuando mtry es mayor o igual 15, R nos indica lo siguiente: "invalid mtry: reset to within valid range". Esto quiere decir que, cuando tomamos estos valores, R nos indica que no son válidos. Por tanto, lo que hace el software es colocar a mtry con el valor por defecto. Dado que no es de interés del ejercicios utilizar el valor de mtry por defecto, solo estamos considerando los valores válidos para mtry: 5 y 10.

```{r}

library(randomForest)

## training and testing data
set.seed (1)

train <- sample (1: nrow(Boston), nrow(Boston) / 2)

boston.test <- Boston[-train, "medv"]

## lists

#usamos solo los valores de 5 y 10 para mtry. Esto debido a que, cuando se usan valores desde 15 en adelante, R bota el siguiente error: "invalid mtry: reset to within valid range"

mtry_list <- as.list( seq(5, 10, by = 5) ) 

ntree_list <- as.list( seq(200, 800, by = 100) )

df = expand.grid(mtry_list, ntree_list)
        
## number of possible combinations
n_combinations <- nrow( expand.grid(mtry_list, ntree_list) )

## MSE vector
MSE_vector <- rep(0, n_combinations)


## Loop que combina mtry = (5,10) con ntree = (200,300,400,500,600,700,800)

for(i in seq_len(nrow(df))) {

        
  bag.boston <- randomForest(medv ∼ ., data = Boston ,
                           subset = train , 
                           mtry = unlist( df[i,][1]  ),
                           ntree = unlist( df[i,][2]  ))
  
  
  yhat.bag <- predict(bag.boston , newdata = Boston[-train , ])
  
  MSE_vector[i] <- mean (( yhat.bag - boston.test)^2) 
    
    
}

MSE_vector

```



```{r}

mtry_mtree <- c("(5,200)","(5,300)","(5,400)","(5,500)","(5,600)","(5,700)","(5,800)",
            "(10,200)","(10,300)","(10,400)","(10,500)","(10,600)","(10,700)","(10,800)")

error_prueba <- MSE_vector

MSE_df = data.frame(mtry_mtree, error_prueba )

MSE_df = rownames_to_column(MSE_df)


ggplot(MSE_df, 
       aes(x=mtry_mtree, y=error_prueba)) +
  geom_bar(stat="identity") +
  theme(axis.text.x=element_text(angle=-90, vjust=0.5, hjust=0))

```

Si ejecuta el siguiente code chunck, podrá visualizar el error/warning que ocurre cuando mtry está entre el rango de 5 a 40 con saltos de 5


```{r}

mtry_list <- as.list( seq(5,40, by = 5) ) 

ntree_list <- as.list( seq(200, 800, by = 100) )

df = expand.grid(mtry_list, ntree_list)
        
## number of possible combinations
n_combinations <- nrow( expand.grid(mtry_list, ntree_list) )

## MSE vector
MSE_vector <- rep(0, n_combinations)


for(i in seq_len(nrow(df))) {

        
  bag.boston <- randomForest(medv ∼ ., data = Boston ,
                           subset = train , 
                           mtry = unlist( df[i,][1]  ),
                           ntree = unlist( df[i,][2]  ))
  
  
  yhat.bag <- predict(bag.boston , newdata = Boston[-train , ])
  
  MSE_vector[i] <- mean (( yhat.bag - boston.test)^2) 
    
    
}

```


**5) Objetivo: buscaremos predecir las ventas de asientos de automóvil, de manera cuantitativa**

a) Divida el conjunto de datos en un conjunto de entrenamiento y un conjunto de prueba.

```{r, include=FALSE}

#install.packages( "gbm" )
#install.packages( "tree" )
#install.packages( "ISLR2" )
#install.packages( "BART" )
#install.packages( "randomForest")
#install.packages( "Carseats")

library(gbm)
library(BART)

#### Importamos la data:
library(tree)
library(ISLR2)
attach(Carseats)
View(Carseats)
```

```{r}
set.seed (1)
train = sample (1: nrow(Carseats), nrow(Carseats) / 2)
test = (1:nrow(Carseats))[-train]
```

```{r}
train
```


```{r}
test
```


b) Ajuste un arbol de regresion al conjunto de entrenamiento. Interprete los resultados. ¿Que valor de MSE obtienes para el conjunto de prueba?

```{r}
tree.Carseats = tree(Sales ∼., Carseats , subset = train)
tree.Carseats.sum = summary(tree.Carseats)
tree.Carseats.sum
```


```{r}
#Observamos el grafico: 
plot(tree.Carseats)
text(tree.Carseats , pretty = 0)
```

Respuesta: 
En este caso, solo se emplearon 6 variables para construir el arbol. Estas variables son la calidad de la ubicación de las estanterías (Shelveloc),el precio de los asientos en cada lugar (Price), la edad de la población (Age) , el presupuesto de publicidad local (Advertising), el precio cobrado por el competidor en cada ubicación (Comprice) y la variable US que indica si la tienda se ubica en USA o no.

```{r}
used_vars <- as.character(tree.Carseats.sum$used)
used_vars
```

Además,se tienen 18 nodos terminales en el árbol. 

En lo que respecta a la evaluación del modelo, este presenta un Residual mean deviance de: 4.922039

```{r, INCLUDE = FALSE}
Carseats.test = predict( tree.Carseats, newdata=Carseats[test,] )
test.MSE = mean( ( Carseats.test - Carseats[test,]$Sales )^2 )
print( test.MSE )
```


c) Utilice la validacion cruzada para determinar el nivel optimo de complejidad del arbol. ¿Podra el arbol mejorar el MSE del conjunto de prueba?

```{r}
cv.Carseats <- cv.tree(tree.Carseats)
plot(cv.Carseats$size, cv.Carseats$dev, type = "b", xlim = c(4, 18))
```
Seleccionamos el mejor nivel de complejidad del arbol. Ahora empleamos ese nivel de nodos para el árbol:  

```{r}
prune.Carseats <- prune.tree(tree.Carseats , best = 18)
plot(prune.Carseats)
text(prune.Carseats , pretty = 0)
```

Respuesta: 
El tamaño óptimo será de 18 nodos terminales 

Al probar el modelo en la data de prueba, se observa que el MSE asociado al árbol de decisión se mantienen en: 4.922039

```{r}
yhat <- predict(tree.Carseats , newdata = Carseats[-train , ])
Carseats.test <- Carseats[-train, "Sales"]
plot(yhat , Carseats.test)
abline (0, 1)
mean (( yhat - Carseats.test)^2)
```


d) Utilice el metodo bagging para analizar estos datos. ¿Que valor de MSE obtienes para el conjunto de prueba? Use la funcion importance() para determinar que variables son las mas importantes.
```{r}
dim(Carseats)
```

```{r, INCLUDE = FALSE}
#install.packages( "randomForest" )
set.seed (1)
library(randomForest)
bag.Carseats <- randomForest(Sales ∼ ., data = Carseats, subset = train , mtry = 11, importance = TRUE)
summary(bag.Carseats)
```


Evaluamos el performance del modelo:

```{r, INCLUDE = FALSE}
Carseats.test <- Carseats[-train, "Sales"]

yhat.bag <- predict(bag.Carseats , newdata = Carseats[-train , ])
plot(yhat.bag , Carseats.test)
abline (0, 1)
mean (( yhat.bag - Carseats.test)^2)
```

Identificamos  la importancia de los predictores del modelo generado:

```{r}
importance.bag = importance( bag.Carseats )
print( importance.bag[ order( importance.bag[,1] ), ] )
```


Respuesta: 
Se observa que el MSE asociado a este arbol es de 2.605253

Además, se identifica que las variables más importantes del modelo considerando  el aumento en el %IncMSE, son: el precio de las sillas (price), la calidad de la ubicación de la estantería (Shelveloc) y el precio del competidor en la ubicación (CompPrice). Es decir, que cuando se excluyen estas variables del modelo, iniciando por la calidad de la ubicación de las estanterías (ShelveLoc), se genera un aumento significativo del MSE. Esto se observa en la siguiente tabla:  

```{r}
importance.bag
```


e) Utilice Random forest para analizar estos datos. ¿Que valor de MSE obtienes para el conjunto de prueba? Use la funcion importance() para determinar que variables son las mas importantes.Describa el efecto de m, el numero de variables consideradas en cada division, sobre la tasa de error obtenida.

Sabemos que por default randomForest() emplea p/3 variables. En este ejercicio se emplearán mtry = 3, donde m= √p. 

```{r}
set.seed (1)
rf.Carseats <- randomForest(Sales ∼ ., data = Carseats ,
                          subset = train , mtry = 3, importance = TRUE)
yhat.rf <- predict(rf.Carseats, newdata = Carseats[-train , ])
mean (( yhat.rf - Carseats.test)^2)
```
Observamos que con m = 3, tenemos un Test MSE  de 2.960559

Vemos la importancia de las variables: 

```{r}
importance.ran = importance(rf.Carseats)
importance.ran
```

A su vez, al medir la importancia de las variables mediente el aumento del %IncMSE, resaltan: Price, Shelveloc, CompPrice. Es decir, al quitar a estas variables, el MSE del modelo aumenta.  En el caso de la variable Price, el MSE aumenta en un 34.97%.  Esto se observa en el siguiente gráfico: 

```{r}
varImpPlot(rf.Carseats)
```
A su vez, para medir el efecto de m, el numero de variables consideradas en cada division, sobre la tasa de error obtenida, se plantea: 

Aumentar el número de variables para identificar su efecto en la tasa de error obtenida: 

   1.  Primero trabajamos con 6 variables: 

```{r}
set.seed (1)
rf.Carseats <- randomForest(Sales ∼ ., data = Carseats ,
                          subset = train , mtry = 6, importance = TRUE)
yhat.rf <- predict(rf.Carseats, newdata = Carseats[-train , ])
mean (( yhat.rf - Carseats.test)^2)
```
MSE:  Vemos que conforme aumenta el tamaño de m, reduce el MSE. En este caso es de 2.667767

  2. Ahora aumentamos el tamaño de m a 10.  

```{r}
set.seed (1)
rf.Carseats10 <- randomForest(Sales ∼., data = Carseats , subset = train , mtry = 10, importance = TRUE)
yhat.rf10 <- predict(rf.Carseats10, newdata = Carseats[-train , ])
mean (( yhat.rf10 - Carseats.test)^2)
```
MSE: Vemos que el MSE se reduce a 2.605253


Respuesta: 
El MSE en el grupo de testeo del modelo con mtry = 3, es de 2.960559. Vemos una reducción en el MSE respecto a Bagging, cuando el modelo incluye un m = 6 . Pero el m= 10 presenta el mejor MSE al ser de 2.605 

De igual forma, como se observó en el ejercicio, al aumentar el tamaño de m de 3 a 10 variables en el modelo, el MSE se va reduciendo progresivamente. 

f) Analice la data utilizando boosting:

```{r}
set.seed (1)
boost.Carseats <- gbm(Sales ∼ ., data = Carseats[train , ],
                    distribution = "gaussian", n.trees = 5000,
                    interaction.depth = 4)
summary(boost.Carseats)
```


Analizamos el performance del modelo: 

```{r}
yhat.boost <- predict(boost.Carseats ,
                      newdata = Carseats[-train , ], n.trees = 5000)
mean (( yhat.boost - Carseats.test)^2)
```
Respuesta: Al emplear boosting, que posee un proceso de aprendizaje continuo con árboles ligados, se observa que el MSE se reduce a 1.806206. Esto implica una mejora en la capacidad predictiva del modelo que emplea Boosting por sobre Random Forests y Bagging.

Asimismo, se observa que las variables Price (precio) y Sherveloc (la calidad de la ubicación de las estanterías) son las más importantes del modelo. 

