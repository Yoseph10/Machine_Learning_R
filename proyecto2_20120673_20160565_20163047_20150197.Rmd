---
title: "Proyecto 2"
#author: ""
#date: "3/15/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Integrantes:**

Luciana Figueroa Valdivia (20120673),
Chiara Zamora Mendoza (20160565),
Yoseph Ayala Valencia (20163047),
Claudia Villena Tagle (20150197)


```{r}
#loading libraries
library(ISLR2)
library(glmnet)
library(MASS)
library(boot)
```

**1)   **

1. Considere el índice de Gini, el error de clasificación y la entropía en un árbol de clasificación con
dos clases. Cree una sola gráfica que muestre cada una de estas cantidades como una función de
pˆm1. El eje x debe mostrar pˆm1, con un rango de 0 a 1, y el eje y debe mostrar el valor del índice
de Gini, el error de clasificación y la entropía.

* *Índice de Gini*

$$G=\sum_{k=1}^K\hat p_{mk}(1 - \hat p_{mk})$$

* *Entropía*

$$D=-\sum_{k=1}^K\hat p_{mk}\text{log} \hat p_{mk}$$

```{r}
p = seq(0, 1, 0.01)

indice_gini = 2 * p * (1 - p)

error_clasificacion = 1 - pmax(p, 1 - p)

entropia = - (p * log(p) + (1 - p) * log(1 - p))
```

```{r}
matplot(p, 
        cbind(indice_gini, error_clasificacion, entropia), 
        pch = c(1,2,3),
        col = c("darkblue", "brown", "darkolivegreen4"),
        ylab = "Gini index, class. error, entropy",
        type = 'b')
legend('bottom', inset=.01, legend = c('Índice de Gini', 'Error de clasificación', 'Entropía'), 
       col = c("darkblue", "brown", "darkolivegreen4" ), 
       pch=c(1,2,3))
```

**2)   **

**Enfoque del voto mayoritario**

Consideraremos como punto de corte al 0.5. Es decir, si es que hay más cantidad de valores que sean mayor o igual a 0.5, ello significa que el voto mayoritario indica que la predicción nos da rojo. Si es que es menor a 0.5, la predicción nos da verde.
Encontramos que 6 valores son mayores o iguales a nuestro punto de corte (0.5) y que 4 son menores. De esta forma, 6 > 4 lo que implica que, bajo este enfoque, la predicción nos debe dar rojo.

**Enfoque de probabilidad media**

Para este enfoque, solo debemos promediar los valores. Realizando ello, obtenemos un valor de 0.45. Al ser este menor que nuestro punto de corte (0.5), nos indicaría que la predicción es verde bajo este enfoque.


```{r}
#creamos el vector con el conjunto de valores
r <-  c(0.1, 0.15, 0.2, 0.2, 0.55, 0.6, 0.6, 0.65, 0.7, 0.75)

#Enfoque del voto mayoritario
R <- sum(r >= 0.5)
R

G <- sum(r < 0.5)
G

R > G #TRUE

#Enfoque de probabilidad media
mean(r) #0.45

```


**3)   **
Para el caso de los árboles de decisión tenemos dos posibilidades, los de clasificación y los de regresión. En el caso de clasificación usamos el índice de gini o el de entropía como criterio de decisión para las particiones óptimas que se harán en el árbol.<br/>
En el caso de los arboles de regresión, que es en el cual nos concentraremos, se usa mínimos cuadrados. Buscamos las particiones que reduzcan la suma de cuadrados de los residuos, o en inglés _Residual Sum Square (RSS)_, entre la observación y la media de cada nodo encontrado. 
Dado el número de variables que normalmente se usan para este algoritmo, es casi imposible tomar en cuenta todas las particiones que se pueden armar, por eso se hace uso de un método de partición recursivo, denominado en inglés como _recursive binary splitting_. <br/>
Lo que hace este método es comenzar desde el tope del árbol y selecciona la variable que dividirá a la base en dos grupos, la variable seleccionada será aquella que reduzca el _RSS_; Luego de encontrar la variable ideal y tener dos niveles nuevos, pasa a abrir estos dos nodos hallados y volver a dividirlos en dos a cada uno, hallando una nueva variable que siga minimizando el _RSS_. Lo importante de este método es que es recursivo ya que va a intentar todas las variables posibles en el *mismo* nivel, es decir, no pasará a la siguiente apertura de los nodos si es que no está seguro de haber encontrado a variable adecuada para la partición. <br/>
Este proceso se repetirá hasta que quede un número mínimo de observaciones dentro de los nodos finales. <br/>
Al tener el árbol final tenemos que tomar en cuenta que se puede presentar el problema de sobreajuste. Dada a la cantidad de variables y sub conjuntos de árboles que se pueden obtener dentro del árbol obtenido por el método de _recursive binary splitting_, es muy costoso hacer validación cruzada, es por eso que el paso final es hacer un podado del árbol, denominado en inglés como _pruning_, que introduce un parámetro $\alpha$ que permite que se encuentre los mejores subsets de árboles que obtienen el mejor ajuste de la data de entrenamiento con la menor cantidad de niveles posibles. Para hallar esto, se busca reducir la ecuación de:

$$\sum_{m=i}^{|T|}\sum_{i:x_i \in R_m} (y_i - \hat{y}_{R_m})^2 + \alpha|T|$$

En este caso, $|T|$ es el número de nodos terminales del arbol $T$, considerando que si $\alpha$ = 0 tendríamos el árbol obtenido originalmente, a medida que aumente $\alpha$ el árbol se "podará" más. El $R_m$ es la partición correspondiente al nodo terminal $m$ y el $\hat{y}_{R_m}$ es la media de las observaciones del set de entrenamiento que se encuentran en la partición $R_m$.<br/>
Para obtener el $\alpha$ ideal, haremos una validación de k-fólios y al obtener este, volvemos al dataset completo para obtener el árbol podado correspondiente a este $\alpha$ ideal. 


**4)   **


```{r}

library(MASS)
library(randomForest)

set.seed(1)

#empezamos a crear train y tests simples

n <- nrow(Boston)
train.set <- sample(1:n,n/2)

Boston.train <- Boston[train.set, -14]
Boston.test <- Boston[-train.set, -14]

Y.train <- Boston[train.set, 14]
Y.test <- Boston[-train.set, 14]

#Creamos los distintos random forest con distintos mtry

rf.a <- randomForest(Boston.train , Y.train, Boston.test, Y.test, mtry = ncol(Boston.train), ntree = 5000)
rf.b <- randomForest(Boston.train , Y.train, Boston.test, Y.test, mtry = ncol(Boston.train) / 2, ntree = 5000)
rf.c <- randomForest(Boston.train , Y.train, Boston.test, Y.test, mtry = sqrt(ncol(Boston.train)), ntree = 5000)

#Gráfico que muestra los errores de prueba

library(ggplot2)
ggplot() + 
  geom_line(aes(x = c(1:5000), y = rf.a$test$mse, color = "M = P")) +
  geom_line(aes(x = c(1:5000), y = rf.b$test$mse, color = "M = P/2")) +
  geom_line(aes(x = c(1:5000), y = rf.c$test$mse, color = "M = Sqrt(P)")) +
  theme_classic() + 
  xlab("Number of Trees") + ylab("Test Classification Error") + scale_colour_discrete(name="Form")

rf.a$test$mse[5000]

rf.b$test$mse[5000]

rf.c$test$mse[5000]


```

**5) Objetivo: buscaremos predecir las ventas de asientos de automóvil, de manera cuantitativa**

a) Divida el conjunto de datos en un conjunto de entrenamiento y un conjunto de prueba.

```{r, include=FALSE}

#install.packages( "gbm" )
#install.packages( "tree" )
#install.packages( "ISLR2" )
#install.packages( "BART" )
#install.packages( "randomForest")
#install.packages( "Carseats")

library(gbm)
library(BART)

#### Importamos la data:
library(tree)
library(ISLR2)
attach(Carseats)
View(Carseats)
```

```{r}
set.seed (1)
train = sample (1: nrow(Carseats), nrow(Carseats) / 2)
test = (1:nrow(Carseats))[-train]
```

```{r}
train
```


```{r}
test
```


b) Ajuste un arbol de regresion al conjunto de entrenamiento. Interprete los resultados. ¿Que valor de MSE obtienes para el conjunto de prueba?

```{r}
tree.Carseats = tree(Sales ∼., Carseats , subset = train)
tree.Carseats.sum = summary(tree.Carseats)
tree.Carseats.sum
```


```{r}
#Observamos el grafico: 
plot(tree.Carseats)
text(tree.Carseats , pretty = 0)
```

Respuesta: 
En este caso, solo se emplearon 6 variables para construir el arbol. Estas variables son high, la calidad de la ubicación de las estanterías (Shelveloc), el precio cobrado por el competidor en cada ubicación (Comprice), la población regional (Population), el precio de los asientos en cada lugar (Price) y la edad de la población (Age) 

```{r}
used_vars <- as.character(tree.Carseats.sum$used)
used_vars
```

Además,se tienen 11 nodos terminales en el árbol. 

En lo que respecta a la evaluación del modelo, este presenta un Residual mean deviance de: 2.736873

```{r, INCLUDE = FALSE}
Carseats.test = predict( tree.Carseats, newdata=Carseats[test,] )
test.MSE = mean( ( Carseats.test - Carseats[test,]$Sales )^2 )
print( test.MSE )
```


c) Utilice la validacion cruzada para determinar el nivel optimo de complejidad del arbol. ¿Podra el arbol mejorar el MSE del conjunto de prueba?

```{r}
cv.Carseats <- cv.tree(tree.Carseats)
plot(cv.Carseats$size, cv.Carseats$dev, type = "b", xlim = c(2, 12))
```
Seleccionamos el mejor nivel de complejidad del arbol. Ahora empleamos ese nivel de nodos para el árbol:  

```{r}
prune.Carseats <- prune.tree(tree.Carseats , best = 3)
plot(prune.Carseats)
text(prune.Carseats , pretty = 0)
```

Respuesta: 
El tamaño óptimo será de 3 nodos terminales 

Al probar el modelo en la data de prueba, se observa que el MSE asociado al árbol de decisión es:   2.736873

```{r}
yhat <- predict(tree.Carseats , newdata = Carseats[-train , ])
Carseats.test <- Carseats[-train, "Sales"]
plot(yhat , Carseats.test)
abline (0, 1)
mean (( yhat - Carseats.test)^2)
```


d) Utilice el metodo bagging para analizar estos datos. ¿Que valor de MSE obtienes para el conjunto de prueba? Use la funcion importance() para determinar que variables son las mas importantes.
```{r}
dim(Carseats)
```

```{r, INCLUDE = FALSE}
#install.packages( "randomForest" )
set.seed (1)
library(randomForest)
bag.Carseats <- randomForest(Sales ∼ ., data = Carseats, subset = train , mtry = 12, importance = TRUE)
summary(bag.Carseats)
```


Evaluamos el performance del modelo:

```{r, INCLUDE = FALSE}
Carseats.test <- Carseats[-train, "Sales"]

yhat.bag <- predict(bag.Carseats , newdata = Carseats[-train , ])
plot(yhat.bag , Carseats.test)
abline (0, 1)
mean (( yhat.bag - Carseats.test)^2)
```

Identificamos  la importancia de los predictores del modelo generado:

```{r}
importance.bag = importance( bag.Carseats )
print( importance.bag[ order( importance.bag[,1] ), ] )
```


Respuesta: 
Se observa que el MSE asociado a este arbol es de 2.162261

Además, se identifica que las variables más importantes del modelo considerando  el aumento en el %IncMSE, son: High, la calidad de la ubicación de la estantería (Shelveloc), el precio de las sillas (price) y el precio del competidor en la ubicación (CompPrice). Es decir, que cuando se excluyen estas variables del modelo, iniciando por la calidad de la ubicación de las estanterías (ShelveLoc), se genera un aumento significativo del MSE. Esto se observa en la siguiente tabla:  

```{r}
importance.bag
```


e) Utilice Random forest para analizar estos datos. ¿Que valor de MSE obtienes para el conjunto de prueba? Use la funcion importance() para determinar que variables son las mas importantes.Describa el efecto de m, el numero de variables consideradas en cada division, sobre la tasa de error obtenida.

Sabemos que por default randomForest() emplea p/3 variables. En este ejercicio se emplearán mtry = 4, donde m= √p. 

```{r}
set.seed (1)
rf.Carseats <- randomForest(Sales ∼ ., data = Carseats ,
                          subset = train , mtry = 4, importance = TRUE)
yhat.rf <- predict(rf.Carseats, newdata = Carseats[-train , ])
mean (( yhat.rf - Carseats.test)^2)
```
Observamos que con m = 4, tenemos un Test MSE  de 2.12137

Vemos la importancia de las variables: 

```{r}
importance.ran = importance(rf.Carseats)
importance.ran
```

A su vez, al medir la importancia de las variables mediente el aumento del %IncMSE, resaltan: High, Price, Shelveloc, CompPrice. Es decir, al quitar a estas variables, el MSE del modelo aumenta.  En el caso de High, el MSE aumenta en un 64.16%.  Esto se observa en el siguiente gráfico: 

```{r}
varImpPlot(rf.Carseats)
```
Aumentamos el número de variables para identificar su efecto en la tasa de error obtenida: 

Primero trabajamos con 6 variables: 
```{r}
set.seed (1)
rf.Carseats <- randomForest(Sales ∼ ., data = Carseats ,
                          subset = train , mtry = 6, importance = TRUE)
yhat.rf <- predict(rf.Carseats, newdata = Carseats[-train , ])
mean (( yhat.rf - Carseats.test)^2)
```
MSE:  Vemos que conforme aumenta el tamaño de m, también lo hace el MSE. En este caso sube a 2.176362

Ahora aumentamos el tamaño de m a 10.  

```{r}
set.seed (1)
rf.Carseats10 <- randomForest(Sales ∼., data = Carseats , subset = train , mtry = 10, importance = TRUE)
yhat.rf10 <- predict(rf.Carseats10, newdata = Carseats[-train , ])
mean (( yhat.rf10 - Carseats.test)^2)
```
MSE: Vemos que el MSE se reduce a 2.152985


Respuesta: 
El MSE en el grupo de testeo es de 2.667767. Vemos una reducción en el MSE respecto a Bagging, cuando el modelo incluye un m = 10 . Pero el m= 4 presenta el mejor MSE al ser de 2.12137. 



De igual forma, como se observó en el ejercicio, al aumentar el tamaño de m de 6 a 10 variables en el modelo, el MSE varia, subiendo cuando el m= 6 y bajando con m= 10. 

f) Analice la data utilizando boosting:

```{r}
set.seed (1)
boost.Carseats <- gbm(Sales ∼ ., data = Carseats[train , ],
                    distribution = "gaussian", n.trees = 5000,
                    interaction.depth = 4)
summary(boost.Carseats)
```


Analizamos el performance del modelo: 

```{r}
yhat.boost <- predict(boost.Carseats ,
                      newdata = Carseats[-train , ], n.trees = 5000)
mean (( yhat.boost - Carseats.test)^2)
```
Respuesta: Al emplear boosting, que posee un proceso de aprendizaje continuo con árboles ligados, se observa que el MSE se reduce a 1.740801. Esto implica una mejora en la capacidad predictiva del modelo que emplea Boosting por sobre Random Forests y Bagging.

Asimismo, se observa que las variables, High, Price (precio) y Sherveloc (la calidad de la ubicación de las estanterías) son las más importantes.